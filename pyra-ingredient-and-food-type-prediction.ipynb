{"cells":[{"metadata":{"id":"Lj_Ai5nrE7bs"},"cell_type":"markdown","source":"This notebook, which is my submission for WinHacks 2021, will detail the training of the Pyra Deep Learning Bot.\n\nTLDR Implementation Details:\n- ResNet200D, to Encode Images, leveraged Transfer Learning\n- MultiClass Classification Problem"},{"metadata":{"id":"XPrI1hRjPsZ9"},"cell_type":"markdown","source":"# Load Dependencies and the Dataset(Recipes-5k)"},{"metadata":{"id":"xnArspL8HilF","trusted":true,"scrolled":false},"cell_type":"code","source":"%%capture \n# Various Dependencies from pytorch \nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torchvision\n\n# Other handy imports\nimport numpy as np\nimport pandas as pd\nimport copy\nimport math\nimport random\n!pip install livelossplot\nimport livelossplot\n!pip install timm\nimport timm\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import ShuffleSplit\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport tqdm.notebook as tqdm\nimport cv2\nimport PIL\nimport re\n# Set up GPU device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","execution_count":1,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Reproducibility:"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"import os\nimport random\nseed = 42\nrandom.seed(seed)\nos.environ['PYTHONHASHSEED'] = str(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False","execution_count":2,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"def seed_worker(worker_id):\n    worker_seed = torch.initial_seed() % 2**32\n    numpy.random.seed(worker_seed)\n    random.seed(worker_seed)","execution_count":3,"outputs":[]},{"metadata":{"id":"HVxVYe4Mi1oK"},"cell_type":"markdown","source":"# Process Dataset"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"%%capture\n!conda install -y gdown\n!gdown https://drive.google.com/uc?id=11ojDNqjowZIf9RzLWc25_xnoHefDvSBS\n!unzip ./Recipes5k.zip\n!rm -f ./Recipes5k.zip","execution_count":4,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# HYPER PARAMETERS\nbase_path = './Recipes5k/images/'\ntrain_path = \"./Recipes5k/annotations/train_images.txt\"\nval_path = \"./Recipes5k/annotations/val_images.txt\"\ntest_path = \"./Recipes5k/annotations/test_images.txt\"\ntrain_labels = './Recipes5k/annotations/train_labels.txt'\nval_labels = './Recipes5k/annotations/val_labels.txt'\ntest_labels = './Recipes5k/annotations/test_labels.txt'\nground_truth_labels = \"./Recipes5k/annotations/ingredients_simplified_Recipes5k.txt\"","execution_count":5,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"def process_line(line):\n    page= str.strip(line)\n    idx = 0\n    for i in range(len(page)):\n        if page[i] == '/':\n            idx = i\n            break\n    return page, page[:idx]\ndef load_all():\n    TRAIN_IMAGES = []\n    TRAIN_CLASSES = []\n    TRAIN_GTS = []\n    GROUND_TRUTHS = []\n    UNIQUE_INGREDIENTS = []\n    # Load in All Images\n    with open(train_path, \"r\") as file:\n        for line in file:\n            page, class_val = process_line(line)\n            TRAIN_IMAGES += [page]\n            TRAIN_CLASSES += [class_val]\n                \n    with open(val_path, \"r\") as file:\n        for line in file:\n            page, class_val = process_line(line)\n            TRAIN_IMAGES += [page]\n            TRAIN_CLASSES += [class_val]\n    with open(test_path, 'r') as file:\n        for line in file:\n            page, class_val = process_line(line)\n            TRAIN_IMAGES += [page]\n            TRAIN_CLASSES += [class_val]\n    # Load in Ground Truth Ingredients\n    with open(ground_truth_labels, \"r\") as file:\n        for line in file:\n            vals = str.lower(str.strip(line)).split(',')\n            GROUND_TRUTHS += [vals]\n            UNIQUE_INGREDIENTS += vals\n    UNIQUE_INGREDIENTS = sorted(list(set(UNIQUE_INGREDIENTS)))\n    # Load in True Values\n    with open(train_labels, 'r') as file:\n        for line in file:\n            val = int(str.strip(line))\n            TRAIN_GTS +=[GROUND_TRUTHS[val]]\n    with open(val_labels, \"r\") as file:\n        for line in file:\n            val = int(str.strip(line))\n            TRAIN_GTS += [GROUND_TRUTHS[val]]\n    with open(test_labels, \"r\") as file:\n        for line in file:\n            val = int(str.strip(line))\n            TRAIN_GTS += [GROUND_TRUTHS[val]]\n    return TRAIN_IMAGES, TRAIN_CLASSES, sorted(list(set(TRAIN_CLASSES))), TRAIN_GTS, UNIQUE_INGREDIENTS","execution_count":6,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"Train_Images, Train_Classes, Unique_Classes, Train_GT, Unique_Ingredients = load_all()","execution_count":7,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Training Transforms"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# Hyper Parameters Set\nIMAGE_SIZE = 224\nBATCH_SIZE = 64\nTEST_BATCH_SIZE = 128","execution_count":8,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"train_transforms = A.Compose([\n    A.RandomResizedCrop(IMAGE_SIZE, IMAGE_SIZE, scale=(0.6, 0.6), p=1),\n    A.Flip(p = 0.7),\n    A.OneOf([\n        A.MotionBlur(blur_limit=(3, 5)),\n        A.MedianBlur(blur_limit=5),\n        A.GaussianBlur(blur_limit=(3, 5)),\n        A.GaussNoise(var_limit=(5.0, 30.0)),\n        A.MultiplicativeNoise(),\n    ], p=0.7),\n    A.OneOf([\n        A.OpticalDistortion(distort_limit=1.0),\n        A.GridDistortion(num_steps=5, distort_limit=1.),\n        A.ElasticTransform(alpha=3),\n    ], p=0.7),\n    A.CLAHE(clip_limit=4.0, p=0.7),\n    A.IAASharpen(p=0.5),\n    A.ColorJitter(p = 0.7),\n    A.OneOf([\n        A.ImageCompression(),\n        A.Downscale(scale_min=0.7, scale_max=0.95),\n    ], p=0.2),\n    A.OneOf([\n        A.ToGray(),\n        A.ToSepia()\n    ]),\n    A.CoarseDropout(max_holes=8, max_height=int(IMAGE_SIZE * 0.1),\n                       max_width=int(IMAGE_SIZE* 0.1), p=0.5),\n    A.Cutout(num_holes = 32),\n    A.ShiftScaleRotate(shift_limit=0.2, scale_limit=0.2, rotate_limit=45, border_mode=0, p=0.85),\n    A.Normalize(mean=0.482288, std=0.22085),\n    ToTensorV2()\n])\n\ntest_transforms = A.Compose([\n    A.Resize(IMAGE_SIZE, IMAGE_SIZE),\n    A.Normalize(),\n    ToTensorV2()\n])","execution_count":9,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Custom Dataset"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"class RecipesDataset(torch.utils.data.Dataset):\n    def __init__(self, images, GT, GT_classes, unique_classes, unique_ingredients, transforms, device):\n        self.base_path = base_path # Defined In Hyper Parameter\n        self.images = images\n        self.device = device\n        self.GT = GT\n        self.transforms = transforms\n        self.unique_ingredients = unique_ingredients\n        self.unique_classes = unique_classes\n        self.GT_classes = GT_classes\n        self.num_ingredients = len(self.unique_ingredients)\n        # Create Dictionary Mapping ingredients to keys\n        self.ingredient2idx = {}\n        self.idx2ingredient = {}\n        for idx in range(len(self.unique_ingredients)):\n            self.ingredient2idx[self.unique_ingredients[idx]] = idx\n            self.idx2ingredient[idx] = self.unique_ingredients[idx]\n        # Dictionary Mapping Classes to Keys\n        self.class2idx = {}\n        self.idx2class = {}\n        for idx in range(len(self.unique_classes)):\n            self.class2idx[self.unique_classes[idx]] = idx\n            self.idx2class[idx] = self.unique_classes[idx]\n    def decode_Ing_Prediction(self, predictions):\n        '''\n        Decodes Predictions into a list of ingredients.\n        predictions: Tensor(B, L)\n        '''\n        B, L = predictions.shape\n        batched_ingredients = []\n        for b in range(B):\n            ingredients = []\n            for l in range(L):\n                if predictions[b, l]:\n                    ingredients += [self.idx2ingredient[l]]\n            batched_ingredients += [ingredients]\n        return batched_ingredients\n        \n    def decode_Class_Prediction(self, predictions):\n        '''\n        Decodes Class Predictions into list of class prediction \n        predictions: Tensor(C)\n        '''\n        C = predictions.shape[0]\n        classes_pred = []\n        for val_idx in range(C):\n            classes_pred += [self.class2idx[predictions[val_idx].item()]]\n        return classes_pred\n        \n    def __len__(self):\n        return len(self.images)\n    def __getitem__(self, idx):\n        image_path = self.base_path + self.images[idx]\n        GT = self.GT[idx]\n        GT_class = self.class2idx[self.GT_classes[idx]]\n        # Create GT Tensor(sparse one-hot)\n        ground_truth = torch.zeros((self.num_ingredients), device = self.device)\n        # Encode each ingredient\n        for ingredient in GT:\n            ground_truth[self.ingredient2idx[ingredient]] = 1\n        # Load in Image\n        image = cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        # Augment Image\n        image = self.transforms(image = image)['image']\n        return image, ground_truth, GT_class","execution_count":10,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"splitter = ShuffleSplit(1, test_size = 0.015, train_size = 0.985, random_state = 42)\nfor train_idx, test_idx in splitter.split(Train_Images):\n    break","execution_count":11,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"train_split_Images = [Train_Images[idx] for idx in train_idx.tolist()]\ntrain_split_GT = [Train_GT[idx] for idx in train_idx.tolist()]\ntrain_split_classes = [Train_Classes[idx] for idx in train_idx.tolist()]\n\nval_split_Images = [Train_Images[idx] for idx in test_idx.tolist()]\nval_split_GT = [Train_GT[idx] for idx in test_idx.tolist()]\nval_split_classes = [Train_Classes[idx] for idx in test_idx.tolist()]","execution_count":12,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"Train_Dataset = RecipesDataset(train_split_Images, train_split_GT, train_split_classes, Unique_Classes, Unique_Ingredients, train_transforms, device)\nVal_Dataset = RecipesDataset(val_split_Images, val_split_GT, val_split_classes, Unique_Classes, Unique_Ingredients, test_transforms, device)","execution_count":13,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# Create DataLoaders\nTrain_Dataloader = torch.utils.data.DataLoader(Train_Dataset, batch_size = BATCH_SIZE, shuffle = True, worker_init_fn = seed_worker)\nVal_Dataloader= torch.utils.data.DataLoader(Val_Dataset, batch_size = TEST_BATCH_SIZE, shuffle = False, worker_init_fn = seed_worker)","execution_count":14,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Handy CNN Blocks"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"class ConvBlock(nn.Module):\n    def __init__(self, in_features, out_features, kernel_size, padding, groups):\n        super().__init__()\n        self.conv = nn.Conv2d(in_features, out_features, kernel_size = kernel_size, padding = padding, groups = groups)\n        self.bn = nn.BatchNorm2d(out_features)\n        self.act1 = nn.SiLU(inplace = True)\n    def forward(self, x):\n        return self.bn(self.act1(self.conv(x)))","execution_count":15,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"class SqueezeExcite(nn.Module):\n    def __init__(self, in_channels, inner_channels):\n        super().__init__()\n        self.in_channels = in_channels\n        self.inner_channels = inner_channels\n        self.Squeeze = nn.Linear(self.in_channels, self.inner_channels)\n        self.act1 = nn.SiLU(inplace = True)\n        self.Excite = nn.Linear(self.inner_channels, self.in_channels)\n    def forward(self, x):\n        avg_pool = torch.mean(x, dim = -1)\n        avg_pool = torch.mean(avg_pool, dim = -1)\n        squeezed = self.act1(self.Squeeze(avg_pool))\n        excited = torch.sigmoid(self.Excite(squeezed)).unsqueeze(-1).unsqueeze(-1)\n        return excited * x","execution_count":16,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"class BottleNeck(nn.Module):\n    def __init__(self, in_channels, inner_channels, device):\n        super().__init__()\n        self.device = device \n        self.in_channels = in_channels\n        self.inner_channels = inner_channels\n        \n        self.Squeeze = ConvBlock(self.in_channels, self.inner_channels, 1, 0, 1)\n        self.Process = ConvBlock(self.inner_channels, self.inner_channels, 3, 1, 1)\n        self.Expand = ConvBlock(self.inner_channels, self.in_channels, 1, 0, 1)\n        self.SE = SqueezeExcite(self.in_channels, self.in_channels // 16)\n        self.gamma = nn.Parameter(torch.zeros((1), device = self.device))\n    def forward(self, x):\n        squeezed = self.Squeeze(x) \n        processed = self.Process(squeezed)\n        expanded = self.Expand(processed)\n        excited = self.SE(expanded)\n        return self.gamma * excited + x","execution_count":17,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"class InverseResidualBlock(nn.Module):\n    def __init__(self, in_channels, inner_channels, device):\n        super().__init__()\n        self.in_channels = in_channels\n        self.inner_channels = inner_channels \n        self.device = device\n        \n        self.expand = ConvBlock(self.in_channels, self.inner_channels, 1, 0, 1)\n        self.depthwise = ConvBlock(self.inner_channels, self.inner_channels, 3, 1, self.inner_channels)\n        self.SE = SqueezeExcite(self.inner_channels, self.inner_channels // 16)\n        self.squeeze = ConvBlock(self.inner_channels, self.in_channels, 1, 0, 1)\n        self.gamma = torch.zeros((1), device = self.device)\n    def forward(self, x):\n        expand = self.expand(x)\n        depthwise = self.depthwise(expand)\n        excited = self.SE(depthwise)\n        squeezed = self.squeeze(excited)\n        return self.gamma * squeezed + x","execution_count":18,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\n# Model Part 1: ResNet200D(+ ModifiedParts)"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"class ModifiedResNetQT(nn.Module):\n    def freeze(self, x):\n        for parameter in x.parameters():\n            parameter.requires_grad = False\n    def unfreeze(self, x):\n        for parameter in x.parameters():\n            parameter.requires_grad = True\n    def __init__(self, in_dim, device, drop_prob = 0.1):\n        super().__init__()\n        self.in_dim = in_dim\n        self.drop_prob = drop_prob\n        self.device = device\n        \n        self.model_name = \"resnet200d\"\n        self.model = timm.create_model(self.model_name, pretrained = True)\n        self.model.global_pool = nn.Identity()\n        self.model.fc = nn.Identity()\n        self.freeze(self.model)\n        # Extract Layers\n        self.conv1 = self.model.conv1\n        self.bn1 = self.model.bn1\n        self.act1 = self.model.act1\n        self.pool = self.model.maxpool\n        \n        self.layer1 = self.model.layer1\n        self.layer2 = self.model.layer2\n        self.layer3 = self.model.layer3\n        self.layer4 = self.model.layer4\n        #self.unfreeze(self.layer3)\n        self.unfreeze(self.layer4)\n        \n        \n        self.Attention1 = nn.Identity()#SqueezeExcite(1024, 128)\n        self.Attention2 = SqueezeExcite(2048, 256)\n        self.Attention3 = SqueezeExcite(2048, 256)\n        self.features_extract = nn.Sequential(*[\n            BottleNeck(2048, 512, self.device) for i in range(7)\n        ])\n        self.proj = ConvBlock(2048, self.in_dim, 1, 0, 1)\n        self.dropout = nn.Dropout2d(self.drop_prob)\n        \n    def forward(self, x):\n        features0 = self.pool(self.bn1(self.act1(self.conv1(x))))\n        layer1 = self.layer1(features0)\n        layer2 = self.layer2(layer1)\n        layer3 = self.layer3(layer2)\n        # Attention1\n        layer3 = self.Attention1(layer3)\n        layer4 = self.layer4(layer3)\n        # Attention2\n        layer4 = self.Attention2(layer4)\n        # Dropout\n        layer4 = self.dropout(layer4)\n        # Additional Processing\n        features = self.features_extract(layer4)\n        features = self.Attention3(features)\n        features = self.proj(features)\n        return features\n        ","execution_count":19,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"class FeatureExtractor(nn.Module):\n    def __init__(self, out_dim, device):\n        super().__init__()\n        self.out_dim = out_dim\n        self.device = device\n        self.ResNet = ModifiedResNetQT(self.out_dim, self.device)\n    def forward(self, images):\n        features = self.ResNet(images)\n        return features # (B, 384, 7, 7)","execution_count":20,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"class Pyra_Class(nn.Module):\n    def __init__(self, num_classes, device, drop_prob = 0.2):\n        # File_path: the path to the pretrained feature extractor\n        # Im_size: the size of the image after encoding(7x7 = 49), allows us to precompute positional encodings\n        super().__init__()\n        self.device = device\n        self.drop_prob = drop_prob \n        self.in_dim = 4096\n        self.num_classes = num_classes\n        self.Class_Encoder = FeatureExtractor(self.in_dim, self.device)\n        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.Dropout = nn.Dropout(self.drop_prob)\n        self.ClassLinear = nn.Linear(self.in_dim, self.num_classes) # Class Classification Head\n    def save(self):\n        torch.save(self.state_dict(), \"./Class.pth\")\n    def forward(self, x):\n        encoded_class = self.Class_Encoder(x)\n        processed_class = torch.squeeze(self.avg_pool(encoded_class))\n        if len(processed_class.shape) == 1:\n            processed_class = processed_class.unsqueeze(0)\n        processed_class = self.Dropout(processed_class)\n        Class = self.ClassLinear(processed_class)\n        return Class","execution_count":21,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"class Pyra_Ing(nn.Module):\n    def __init__(self, num_ing_classes, device, drop_prob = 0.2):\n        # File_path: the path to the pretrained feature extractor\n        # Im_size: the size of the image after encoding(7x7 = 49), allows us to precompute positional encodings\n        super().__init__()\n        self.device = device\n        self.drop_prob = drop_prob \n        self.in_dim = 4096\n        self.num_ing_classes = num_ing_classes\n        self.Ing_Encoder = FeatureExtractor(self.in_dim, self.device)\n        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.Dropout = nn.Dropout(self.drop_prob)\n        self.IngLinear = nn.Linear(self.in_dim, self.num_ing_classes) # Ingredient Classification Head\n    def save(self):\n        torch.save(self.state_dict(), \"./Ingredient.pth\")\n    def forward(self, x):\n        encoded_ing = self.Ing_Encoder(x)\n        processed_ing = torch.squeeze(self.avg_pool(encoded_ing))\n        if len(processed_ing.shape) == 1:\n            processed_ing = processed_ing.unsqueeze(0)\n        processed_ing = self.Dropout(processed_ing)\n        IngClass= self.IngLinear(processed_ing)\n        return IngClass","execution_count":22,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"class Pyra(nn.Module):\n    def __init__(self, num_ing_classes, num_classes, device, drop_prob = 0.2):\n        # File_path: the path to the pretrained feature extractor\n        # Im_size: the size of the image after encoding(7x7 = 49), allows us to precompute positional encodings\n        super().__init__()\n        self.device = device\n        self.drop_prob = drop_prob \n        self.num_classes = num_classes\n        self.num_ing_classes = num_ing_classes\n        self.PyraClass = Pyra_Class(self.num_classes, self.device, drop_prob = self.drop_prob)\n        self.PyraIng = Pyra_Ing(self.num_ing_classes, self.device, drop_prob = self.drop_prob)\n    def load_ing(self, path):\n        self.PyraIng.load_state_dict(torch.load(path, map_location = self.device))\n    def load_class(self, path):\n        self.PyraClass.load_state_dict(torch.load(path, map_location = self.device))\n    def save_ing(self):\n        self.PyraIng.save()\n    def save_class(self):\n        self.PyraClass.save()\n    def forward_ing(self, x):\n        return self.PyraIng(x) \n    def forward_class(self, x):\n        return self.PyraClass(x)\n    def forward(self, x):\n        return self.PyraIng(x), self.PyraClass(x)","execution_count":25,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 3: Training"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"class PyraSolver(nn.Module):\n    def __init__(self, num_ing_classes, num_classes, device):\n        super().__init__()\n        self.device = device\n        self.num_ing_classes = num_ing_classes\n        self.num_classes = num_classes\n        self.Pyra = Pyra(self.num_ing_classes, self.num_classes, self.device)\n        self.optim = optim.Adam(self.Pyra.parameters(), lr = 1e-5, weight_decay = 1e-3)\n        self.lr_decay = optim.lr_scheduler.CosineAnnealingLR(self.optim, 5, eta_min = 1e-8)\n        self.IngCriterion = nn.BCEWithLogitsLoss()\n        self.ClassCriterion = nn.CrossEntropyLoss()\n    def forward(self, images):\n        '''\n        Test Time Inference on the Model\n        '''\n        self.eval()\n        with torch.no_grad():\n            IngPred, ClassPred= self.Pyra(images)\n            IngPred = torch.sigmoid(IngPred)\n            ClassPred = F.softmax(ClassPred)\n            selected_ingredients = IngPred >= 0.5\n            _, selected_classes = torch.max(ClassPred, dim = -1)\n            return selected_ingredients, selected_classes\n    def evaluate_class(self, classes, labels):\n        _, classes = torch.max(F.softmax(classes, dim = -1), dim = -1) \n        return torch.sum((classes == labels).int()) / classes.shape[0]\n    def evaluate(self, pred, labels):\n        '''\n        pred: Tensor(B, C)\n        labels: Tensor(B, C)\n        '''\n        pred = torch.sigmoid(pred)\n        bools = pred >= 0.5\n        pred[:, :] = 0\n        pred[bools] = 1\n        \n        ones = labels == 1\n        zeros = labels == 0\n        \n        pred_ones = pred[ones]\n        pred_zeros = pred[zeros] \n        B = pred_zeros.shape[0]\n        acc_zeros = torch.sum((pred_zeros == 0).int()) / B\n        B= pred_ones.shape[0]\n        acc_ones = torch.sum((pred_ones == 1).int()) / B\n        \n        print(f\"Ones: {acc_ones.item()}\")\n        print(f\"Zeros: {acc_zeros.item()}\")\n        return (acc_ones + acc_zeros) / 2\n    def training_jointly(self, trainloader, valloader, NUM_EPOCHS, display_every = 16):\n        '''\n        Jointly Trains two systems. Squeezes a Bit More Performance out of the System.\n        '''\n        liveloss = livelossplot.PlotLosses()\n        bestValAcc = 0\n        for EPOCH in range(NUM_EPOCHS):\n            self.train()\n            logs = {}\n            logs['loss'] = 0\n        \n            count = 0\n            for images, labels, classes in trainloader:\n                self.optim.zero_grad()\n                images = images.to(self.device)\n                labels = labels.to(self.device)\n                classes = classes.to(self.device)\n                \n                IngPred, ClassPred = self.Pyra(images)\n                B, _ = IngPred.shape\n                \n                ones = labels == 1\n                zeros = labels == 0\n                pred_ones = IngPred[ones].unsqueeze(0)\n                pred_zeros = IngPred[zeros].unsqueeze(0)\n                \n                loss_ones = self.IngCriterion(pred_ones, torch.ones_like(pred_ones, device = self.device)) \n                loss_zeros = self.IngCriterion(pred_zeros, torch.zeros_like(pred_zeros, device = self.device))\n                loss_class = self.ClassCriterion(ClassPred, classes)\n                \n                loss = loss_ones + loss_zeros + loss_class\n                loss.backward()\n                self.optim.step()\n                logs['loss'] += loss.item() \n                count += 1\n                del IngPred\n                del ClassPred\n                del images\n                del labels\n                del classes\n                torch.cuda.empty_cache()\n                if count == display_every:\n                    break\n            logs['loss'] /= count\n            \n            self.eval()\n            self.lr_decay.step()\n            with torch.no_grad():\n                logs['val_accuracy'] = 0\n                logs['accuracy'] = 0\n                logs['val_loss'] = 0\n                count = 0\n                for images, labels, classes in valloader:\n                    images = images.to(self.device)\n                    labels = labels.to(self.device)\n                    classes = classes.to(self.device)\n                    \n                    IngPred, ClassPred = self.Pyra(images)\n                    B, _ = IngPred.shape\n                \n                    ones = labels == 1\n                    zeros = labels == 0\n                    pred_ones = IngPred[ones].unsqueeze(0)\n                    pred_zeros = IngPred[zeros].unsqueeze(0)\n\n                    loss_ones = self.IngCriterion(pred_ones, torch.ones_like(pred_ones, device = self.device)) \n                    loss_zeros = self.IngCriterion(pred_zeros, torch.zeros_like(pred_zeros, device = self.device))\n                    loss_class = self.ClassCriterion(ClassPred, classes)\n                    loss = loss_ones + loss_zeros + loss_class\n                    logs['val_loss'] += loss.item()\n                    logs['val_accuracy'] += self.evaluate(IngPred, labels).item()\n                    logs['accuracy'] += self.evaluate_class(ClassPred, classes).item()\n                    count += 1\n                    del IngPred\n                    del ClassPred\n                    del images\n                    del labels\n                    del classes\n                    torch.cuda.empty_cache()\n                logs['val_loss'] /= count\n                logs['val_accuracy'] /= count\n                logs['accuracy'] /= count\n            liveloss.update(logs)\n            liveloss.send()\n            print(f\"E: {EPOCH}, Ing: {round(logs['val_accuracy'], 3)}, Class: {round(logs['accuracy'], 3)}, L: {round(logs['val_loss'], 3)}\")\n            acc = (logs['val_accuracy'] + logs['accuracy']) / 2\n            if acc >= bestValAcc:\n                torch.save(self.state_dict(), \"./BestVal.pth\")\n                bestValAcc = acc\n        torch.save(self.state_dict(),\"./FinalModel.pth\")\n    def train_ing(self, trainloader, valloader, NUM_EPOCHS, display_every = 16):\n        '''\n        Trains Ingredient Portion of Pyra.\n        '''\n        liveloss = livelossplot.PlotLosses()\n        bestValAcc = 0\n        bestValLoss = 999\n        for EPOCH in range(NUM_EPOCHS):\n            logs = {}\n            total_loss = 0\n            count = 0\n            logs['accuracy'] = 0\n            for images, labels, _ in trainloader:\n                self.optim.zero_grad()\n                images = images.to(self.device)\n                labels = labels.to(self.device)\n                \n                IngPred = self.Pyra.forward_ing(images)\n                B, _ = IngPred.shape\n                \n                ones = labels == 1\n                zeros = labels == 0\n                pred_ones = IngPred[ones].unsqueeze(0)\n                pred_zeros = IngPred[zeros].unsqueeze(0)\n                \n                loss_ones = self.IngCriterion(pred_ones, torch.ones_like(pred_ones, device = self.device)) \n                loss_zeros = self.IngCriterion(pred_zeros, torch.zeros_like(pred_zeros, device = self.device)) \n                print('-----------------')\n                print(f\"Ones: {pred_ones.shape}\")\n                print(f\"Zeros: {pred_zeros.shape}\")\n                print(f'Ones Loss: {loss_ones}')\n                print(f'Zeros Loss: {loss_zeros}')\n                loss = loss_ones + loss_zeros\n                logs['accuracy'] += self.evaluate(IngPred, labels).item() \n                loss.backward()\n                self.optim.step()\n                total_loss = total_loss + loss.item()\n                count += 1\n                \n                del images\n                del labels\n                del IngPred\n                torch.cuda.empty_cache() \n                if count == display_every:\n                    break\n            logs['accuracy'] /= count\n            logs['loss'] = total_loss / count\n            self.eval()\n            with torch.no_grad():\n                logs['val_accuracy'] = 0\n                logs['val_loss'] = 0\n                count = 0\n                for images, labels, _ in valloader:\n                    images = images.to(self.device)\n                    labels = labels.to(self.device)\n                    IngPred = self.Pyra.forward_ing(images)\n                    ones = labels == 1\n                    zeros = labels == 0\n                    \n                    \n                    pred_one = IngPred[ones].unsqueeze(0)\n                    pred_zeros = IngPred[zeros].unsqueeze(0)\n                    \n                    loss_ones = self.IngCriterion(pred_one, torch.ones_like(pred_one, device = self.device)).item()\n                    \n                    loss_zeros = self.IngCriterion(pred_zeros, torch.zeros_like(pred_zeros, device = self.device)).item()\n                    loss = loss_ones + loss_zeros\n                    acc = self.evaluate(IngPred, labels)\n                    \n                    \n                    logs['val_accuracy'] += acc.item()\n                    logs['val_loss'] += loss\n                    del images\n                    del labels\n                    del IngPred\n                    torch.cuda.empty_cache()\n                    count += 1\n                logs['val_accuracy'] /= count\n                logs['val_loss'] /= count\n                \n                \n            liveloss.update(logs)\n            liveloss.send()\n    \n            if logs['val_accuracy'] >= bestValAcc:\n                self.Pyra.save_ing()\n                bestValAcc = logs['val_accuracy']\n            print(f\"E: {EPOCH}, L:{round(logs['loss'], 3)}, A: {round(logs['accuracy'], 3)} VA: {round(logs['val_accuracy'], 3)}, VL: {round(logs['val_loss'], 3)}\")\n        torch.save(self.state_dict(), \"FinalModel.pth\")\n    def training_class(self, trainloader, valloader, NUM_EPOCHS, display_every = 16):\n        '''\n        Trains Pyra on Classifying Food.\n        '''\n        liveloss = livelossplot.PlotLosses()\n        bestValAcc = 0\n        bestValLoss = 999\n        saved_model_count = 0\n        for EPOCH in range(NUM_EPOCHS):\n            logs = {}\n            total_loss = 0\n            count = 0\n            logs['accuracy'] = 0\n            logs['loss'] = 0\n            \n            \n            for images, _, classes in trainloader:\n                self.optim.zero_grad()\n                images = images.to(self.device)\n                classes = classes.to(self.device)\n                ClassPred = self.Pyra.forward_class(images)\n                \n                loss = self.ClassCriterion(ClassPred, classes)\n               \n                loss.backward()\n                logs['accuracy'] += self.evaluate_class(ClassPred, classes).item()\n                self.optim.step()\n                total_loss = total_loss + loss.item()\n                count += 1\n                \n                del images\n                del classes\n                del ClassPred\n                torch.cuda.empty_cache() \n                if count == display_every:\n                    break\n            logs['accuracy'] /= count \n            logs['loss'] = total_loss / count\n            \n            \n            self.eval()\n            with torch.no_grad():\n                \n                logs['val_accuracy'] = 0\n                logs['val_loss'] = 0\n                count = 0\n                for images, _, classes in valloader:\n                    images = images.to(self.device)\n                    classes = classes.to(self.device)\n                    ClassPred = self.Pyra.forward_class(images)\n                    \n                    class_loss = self.ClassCriterion(ClassPred, classes)\n                    classAcc = self.evaluate_class(ClassPred, classes)\n                    logs['val_accuracy'] += classAcc.item()\n                    logs['val_loss'] += class_loss.item()\n                    del images\n                    del classes\n                    del ClassPred\n                    torch.cuda.empty_cache()\n                    count += 1\n                logs['val_accuracy'] /= count\n                logs['val_loss'] /= count\n                \n                \n            liveloss.update(logs)\n            liveloss.send()\n            \n            if logs['val_accuracy'] >= bestValAcc:\n                torch.save(self.state_dict(), f\"./BestVal.pth\")\n                bestValAcc = logs['val_accuracy']\n                \n            print(f\"E: {EPOCH}, L:{round(logs['loss'], 3)}, A: {round(logs['accuracy'], 3)} VA: {round(logs['val_accuracy'], 3)}, VL: {round(logs['val_loss'], 3)}\")\n            \n        torch.save(self.state_dict(), \"./FinalModel.pth\")","execution_count":26,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"%%capture\nsolver = PyraSolver(len(Unique_Ingredients), len(Unique_Classes), device) \nsolver.to(device)","execution_count":27,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Load State Dictionaries From Trained"},{"metadata":{"trusted":true},"cell_type":"code","source":"PyraClass = PyraSolver(len(Unique_Ingredients), len(Unique_Classes), device)\nPyraClass.to(device)\nPyraClass.load_state_dict(torch.load(\"../input/pyra-class2/BestVal.pth\", map_location = device))\nsolver.Pyra.PyraClass = copy.deepcopy(PyraClass.Pyra.PyraClass) # Load Model in\ndel PyraClass\ntorch.cuda.empty_cache()","execution_count":29,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"PyraIng = PyraSolver(len(Unique_Ingredients), len(Unique_Classes), device)\nPyraIng.to(device)\nPyraIng.Pyra.load_ing(\"../input/ingredients-trained/Ingredient.pth\")\nsolver.Pyra.PyraIng = copy.deepcopy(PyraIng.Pyra.PyraIng)\ndel PyraIng\ntorch.cuda.empty_cache()","execution_count":30,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Train Models Jointly"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"solver.training_jointly(Train_Dataloader, Val_Dataloader, 20, display_every = 64)","execution_count":40,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 864x576 with 2 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}},{"output_type":"stream","text":"accuracy\n\tvalidation       \t (min:    0.666, max:    0.670, cur:    0.666)\n\ttraining         \t (min:    0.342, max:    0.356, cur:    0.356)\nLoss\n\ttraining         \t (min:    5.344, max:    5.362, cur:    5.362)\n\tvalidation       \t (min:    4.960, max:    4.963, cur:    4.960)\nE: 1, Ing: 0.666, Class: 0.356, L: 4.96\n","name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-40-1e0893e5b732>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_jointly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrain_Dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVal_Dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay_every\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-36-dd03a8b22237>\u001b[0m in \u001b[0;36mtraining_jointly\u001b[0;34m(self, trainloader, valloader, NUM_EPOCHS, display_every)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-10-4479e755de43>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;31m# Augment Image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mground_truth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGT_class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/albumentations/core/composition.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, force_apply, *args, **data)\u001b[0m\n\u001b[1;32m    180\u001b[0m                     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforce_apply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_apply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdual_start_end\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdual_start_end\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/albumentations/core/composition.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, force_apply, **data)\u001b[0m\n\u001b[1;32m    250\u001b[0m             \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m32\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms_ps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforce_apply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/albumentations/core/transforms_interface.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, force_apply, *args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m                     )\n\u001b[1;32m     88\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_with_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/albumentations/core/transforms_interface.py\u001b[0m in \u001b[0;36mapply_with_params\u001b[0;34m(self, params, force_apply, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mtarget_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_target_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0mtarget_dependencies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_dependence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                 \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtarget_dependencies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/albumentations/augmentations/transforms.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, img, random_state, interpolation, **params)\u001b[0m\n\u001b[1;32m   1359\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m             \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapproximate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m         )\n\u001b[1;32m   1363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/albumentations/augmentations/functional.py\u001b[0m in \u001b[0;36mwrapped_function\u001b[0;34m(img, *args, **kwargs)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/albumentations/augmentations/functional.py\u001b[0m in \u001b[0;36melastic_transform\u001b[0;34m(img, alpha, sigma, alpha_affine, interpolation, border_mode, value, random_state, approximate)\u001b[0m\n\u001b[1;32m   1197\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1198\u001b[0m         \u001b[0mdx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgaussian_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1199\u001b[0;31m         \u001b[0mdy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgaussian_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1201\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeshgrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/scipy/ndimage/filters.py\u001b[0m in \u001b[0;36mgaussian_filter\u001b[0;34m(input, sigma, order, output, mode, cval, truncate)\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             gaussian_filter1d(input, sigma, axis, order, output,\n\u001b[0;32m--> 299\u001b[0;31m                               mode, cval, truncate)\n\u001b[0m\u001b[1;32m    300\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/scipy/ndimage/filters.py\u001b[0m in \u001b[0;36mgaussian_filter1d\u001b[0;34m(input, sigma, axis, order, output, mode, cval, truncate)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;31m# Since we are calling correlate, not convolve, revert the kernel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_gaussian_kernel1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcorrelate1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/scipy/ndimage/filters.py\u001b[0m in \u001b[0;36mcorrelate1d\u001b[0;34m(input, weights, axis, output, mode, cval, origin)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ni_support\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_mode_to_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     _nd_image.correlate1d(input, weights, axis, output, mode, cval,\n\u001b[0;32m---> 95\u001b[0;31m                           origin)\n\u001b[0m\u001b[1;32m     96\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.save(solver.state_dict(), \"./Pyra.pth\")","execution_count":33,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save Unique Ingredients\ntorch.save(Unique_Ingredients, \"./Ingredients.pth\")\ntorch.save(Unique_Classes, \"./Classes.pth\")","execution_count":32,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# Delete all Working Files other than PTH \n!rm -rf Recipes5k","execution_count":34,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}